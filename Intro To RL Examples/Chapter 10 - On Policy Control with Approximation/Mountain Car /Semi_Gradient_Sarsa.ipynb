{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "import time\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from tilecoding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the OpenAI Gym Enviroment for MountainCar\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "#Removing the episode limit otherwise the enviroment will automatically end if step count goes to 200\n",
    "env._max_episode_steps = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Description:\n",
    "        The agent (a car) is started at the bottom of a valley. For any given\n",
    "        state the agent may choose to accelerate to the left, right or cease\n",
    "        any acceleration.\n",
    "    Source:\n",
    "        The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
    "    Observation:\n",
    "        Type: Box(2)\n",
    "        Num    Observation               Min            Max\n",
    "        0      Car Position              -1.2           0.6\n",
    "        1      Car Velocity              -0.07          0.07\n",
    "    Actions:\n",
    "        Type: Discrete(3)\n",
    "        Num    Action\n",
    "        0      Accelerate to the Left\n",
    "        1      Don't accelerate\n",
    "        2      Accelerate to the Right\n",
    "        Note: This does not affect the amount of velocity affected by the\n",
    "        gravitational pull acting on the car.\n",
    "    Reward:\n",
    "         Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
    "         on top of the mountain.\n",
    "         Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "    Starting State:\n",
    "         The position of the car is assigned a uniform random value in\n",
    "         [-0.6 , -0.4].\n",
    "         The starting velocity of the car is always assigned to 0.\n",
    "    Episode Termination:\n",
    "         The car position is more than 0.5\n",
    "         Episode length is greater than 200\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.2  -0.07]\n[0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n",
    "\n",
    "POSITION_BOUND = [env.observation_space.low[0],env.observation_space.high[0]]\n",
    "VELOCITY_BOUND = [env.observation_space.low[1],env.observation_space.high[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-1.2, 0.6]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "POSITION_BOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-0.07, 0.07]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "VELOCITY_BOUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [0,1,2] \n",
    "\n",
    "#STARTING STATE = [POSITION,VELOCITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tile Coding Value Function made available by Dr Sutton \n",
    "#Link: - http://incompleteideas.net/tiles/tiles3.html\n",
    "\n",
    "class ValueFunction:\n",
    "\n",
    "    def __init__(self,step_size,number_of_tilings = 8,maxsize = 2048):\n",
    "\n",
    "        self.number_of_tilings = number_of_tilings\n",
    "        self.maxsize =maxsize\n",
    "\n",
    "        self.weights = np.zeros(self.maxsize)\n",
    "\n",
    "        self.step_size = step_size/self.number_of_tilings\n",
    "\n",
    "        self.hashable = IHT(self.maxsize)\n",
    "\n",
    "        self.position_scale = self.number_of_tilings/(POSITION_BOUND[1]-POSITION_BOUND[0])\n",
    "        self.velocity_scale = self.number_of_tilings/(VELOCITY_BOUND[1]-VELOCITY_BOUND[0])\n",
    "\n",
    "    def get_active_tiles(self,state,action):\n",
    "\n",
    "        active_tiles = tiles(self.hashable,self.number_of_tilings,[state[0]*self.position_scale,state[1]*self.velocity_scale],[action])\n",
    "\n",
    "        return active_tiles\n",
    "\n",
    "    #Gets the Value of a State-Action pair using the active tiles\n",
    "    def value(self,state,action):\n",
    "\n",
    "        if state[0] == POSITION_BOUND[1]:\n",
    "            return 0 \n",
    "        else:\n",
    "            active_tiles = self.get_active_tiles(state,action)\n",
    "\n",
    "        # print\n",
    "\n",
    "        return np.sum(self.weights[active_tiles])\n",
    "\n",
    "    #Updates the weights using TD Learning \n",
    "    def Update(self,target,state,action):\n",
    "\n",
    "        active_tiles = self.get_active_tiles(state,action)\n",
    "\n",
    "        estimate = np.sum(self.weights[active_tiles])\n",
    "\n",
    "        delta = self.step_size*(target - estimate)\n",
    "\n",
    "        for active_tile in active_tiles:\n",
    "            self.weights[active_tile] += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.eps = 0.1\n",
    "        # self.eps_decay = 0.997\n",
    "\n",
    "    def choose_action(self,valueFun):\n",
    "\n",
    "        num = np.random.rand()\n",
    "\n",
    "        if num<self.eps:\n",
    "            return np.random.choice(ACTIONS)\n",
    "\n",
    "        values = {}\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            value = valueFun.value(env.state,a)\n",
    "            values[a] = value\n",
    "        # print(values)\n",
    "\n",
    "        max_action = max(values,key = values.get)\n",
    "        # print('\\nMax_Action: ',max_action)\n",
    "\n",
    "        return max_action\n",
    "\n",
    "    def policy(self,valueFun):\n",
    "\n",
    "        values = {}\n",
    "\n",
    "        for a in ACTIONS:\n",
    "            value = valueFun.value(env.state,a)\n",
    "            values[a] = value\n",
    "\n",
    "        max_action = max(values,key = values.get)\n",
    "        return max_action\n",
    "\n",
    "    def run(self,valueFun,iters = 1):\n",
    "\n",
    "        for x in range(iters):\n",
    "\n",
    "            print(\"Predicting State Values: {:.5f}\".format(x), end=\"\\r\")\n",
    "            # self.eps = max(0.1,self.eps*self.eps_decay)\n",
    "\n",
    "            start_state = env.reset()\n",
    "            action = self.choose_action(valueFun)\n",
    "\n",
    "            while True:\n",
    "                state = env.state\n",
    "                next_state,reward,done,_ = env.step(action)\n",
    "                # print('State: ',state)\n",
    "                # print('Next_State: ',next_state)\n",
    "                # time.sleep(1)\n",
    "\n",
    "                if done:\n",
    "                    valueFun.Update(reward,env.state,action)\n",
    "                    break\n",
    "\n",
    "                next_action = self.choose_action(valueFun)\n",
    "                target = reward + valueFun.value(env.state,next_action)\n",
    "                # print(target)\n",
    "                valueFun.Update(target,state,action)\n",
    "\n",
    "                action = next_action\n",
    "\n",
    "        # print(valueFun.weights)\n",
    "        # return valueFun.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tile Coding with step size as 0.2\n",
    "stepsize = 0.2\n",
    "valueFun = ValueFunction(stepsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bike = MountainCar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "source": [
    "#Learning Semi Gradient Sarsa \n",
    "iters = 100\n",
    "Bike.run(valueFun,iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.58576506  0.        ]\n",
      "[-5.86301512e-01 -5.36450986e-04]\n",
      "[-0.58737046 -0.00106895]\n",
      "[-0.58896403 -0.00159357]\n",
      "[-0.5910705  -0.00210647]\n",
      "[-0.59167439 -0.00060388]\n",
      "[-0.59077124  0.00090314]\n",
      "[-0.58836771  0.00240354]\n",
      "[-0.58448145  0.00388625]\n",
      "[-0.57914112  0.00534034]\n",
      "[-0.57238614  0.00675498]\n",
      "[-0.56426656  0.00811958]\n",
      "[-0.55484273  0.00942383]\n",
      "[-0.54418491  0.01065782]\n",
      "[-0.53237279  0.01181212]\n",
      "[-0.51949488  0.01287791]\n",
      "[-0.50564774  0.01384714]\n",
      "[-0.49093517  0.01471257]\n",
      "[-0.47546719  0.01546798]\n",
      "[-0.45935894  0.01610825]\n",
      "[-0.44272952  0.01662943]\n",
      "[-0.42570068  0.01702884]\n",
      "[-0.40839556  0.01730511]\n",
      "[-0.39093737  0.01745819]\n",
      "[-0.37344809  0.01748929]\n",
      "[-0.35604723  0.01740085]\n",
      "[-0.33885076  0.01719647]\n",
      "[-0.32197004  0.01688072]\n",
      "[-0.30551099  0.01645905]\n",
      "[-0.28957338  0.01593761]\n",
      "[-0.27425028  0.0153231 ]\n",
      "[-0.2596277   0.01462258]\n",
      "[-0.24578437  0.01384334]\n",
      "[-0.23279166  0.01299271]\n",
      "[-0.22071367  0.01207799]\n",
      "[-0.20960737  0.01110629]\n",
      "[-0.19952288  0.01008449]\n",
      "[-0.19050375  0.00901913]\n",
      "[-0.18258733  0.00791642]\n",
      "[-0.17780513  0.00478219]\n",
      "[-0.17617563  0.0016295 ]\n",
      "[-0.177705   -0.00152937]\n",
      "[-0.18238744 -0.00468244]\n",
      "[-0.1902049  -0.00781745]\n",
      "[-0.20112628 -0.01092138]\n",
      "[-0.21510621 -0.01397993]\n",
      "[-0.23108341 -0.0159772 ]\n",
      "[-0.24898355 -0.01790014]\n",
      "[-0.2687181  -0.01973455]\n",
      "[-0.29118334 -0.02246524]\n",
      "[-0.31625386 -0.02507052]\n",
      "[-0.3437811  -0.02752724]\n",
      "[-0.37359251 -0.02981141]\n",
      "[-0.40549138 -0.03189887]\n",
      "[-0.43925763 -0.03376625]\n",
      "[-0.47464971 -0.03539208]\n",
      "[-0.5114076  -0.03675788]\n",
      "[-0.54925689 -0.0378493 ]\n",
      "[-0.58791394 -0.03865705]\n",
      "[-0.62709162 -0.03917767]\n",
      "[-0.6655055  -0.03841389]\n",
      "[-0.70288695 -0.03738145]\n",
      "[-0.73898764 -0.03610069]\n",
      "[-0.774583   -0.03559536]\n",
      "[-0.80946887 -0.03488587]\n",
      "[-0.84346404 -0.03399517]\n",
      "[-0.87641181 -0.03294777]\n",
      "[-0.9081806  -0.03176879]\n",
      "[-0.93666367 -0.02848307]\n",
      "[-0.96178294 -0.02511927]\n",
      "[-0.98348384 -0.0217009 ]\n",
      "[-1.00373026 -0.02024642]\n",
      "[-1.02049791 -0.01676765]\n",
      "[-1.03577358 -0.01527567]\n",
      "[-1.04955071 -0.01377713]\n",
      "[-1.06182791 -0.0122772 ]\n",
      "[-1.07260751 -0.0107796 ]\n",
      "[-1.08089438 -0.00828686]\n",
      "[-1.086694   -0.00579963]\n",
      "[-1.08901116 -0.00231716]\n",
      "[-1.08784796  0.0011632 ]\n",
      "[-1.08320333  0.00464463]\n",
      "[-1.07507327  0.00813006]\n",
      "[-1.06345194  0.01162133]\n",
      "[-1.04833359  0.01511835]\n",
      "[-1.02971525  0.01861834]\n",
      "[-1.00760035  0.0221149 ]\n",
      "[-0.98200306  0.02559728]\n",
      "[-0.95295344  0.02904962]\n",
      "[-0.92050308  0.03245036]\n",
      "[-0.88473114  0.03577195]\n",
      "[-0.84575031  0.03898083]\n",
      "[-0.80371229  0.04203802]\n",
      "[-0.75881209  0.0449002 ]\n",
      "[-0.71129059  0.0475215 ]\n",
      "[-0.66143462  0.04985597]\n",
      "[-0.60957408  0.05186053]\n",
      "[-0.55607586  0.05349822]\n",
      "[-0.50133445  0.05474141]\n",
      "[-0.44575989  0.05557456]\n",
      "[-0.38976384  0.05599606]\n",
      "[-0.33374479  0.05601905]\n",
      "[-0.27807391  0.05567089]\n",
      "[-0.2230824  0.0549915]\n",
      "[-0.16905162  0.05403078]\n",
      "[-0.11620617  0.05284545]\n",
      "[-0.06471033  0.05149584]\n",
      "[-0.01466753  0.0500428 ]\n",
      "[0.03387769 0.04854522]\n",
      "[0.08093582 0.04705812]\n",
      "[0.12656727 0.04563145]\n",
      "[0.17087679 0.04430952]\n",
      "[0.21400766 0.04313087]\n",
      "[0.25613632 0.04212866]\n",
      "[0.29746743 0.04133112]\n",
      "[0.33822969 0.04076226]\n",
      "[0.37867224 0.04044255]\n",
      "[0.41906176 0.04038952]\n",
      "[0.45968004 0.04061828]\n",
      "[0.49982186 0.04014182]\n",
      "Episode finished after timesteps\n"
     ]
    }
   ],
   "source": [
    "#Running the OpenAI enviroment with the optimal policy (without epsilon exploration)\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    print(observation)\n",
    "    action = Bike.policy(valueFun)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"Episode finished after timesteps\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}